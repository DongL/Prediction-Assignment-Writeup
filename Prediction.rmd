---
title: "Prediciotn of Activity Patterns Using a Random Forest Model "
author: "Dong Liang"
date: "May 16, 2016"
output: 
  html_document: 
    theme: cerulean
---


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

```

```{r, include = F}
## Load up packages 
library(caret)
library(ggplot2)
library(magrittr)
library(data.table)
library(dplyr)
library(doMC)
library(DescTools)
```


```{r data_retrieval, include = F}
mod = readRDS("/Users/DL/Documents/R project/Github-DongL/Couseraâ€”Practical Machine Learning/model_rf.RDS")
# saveRDS(mod, 'model_rf.RDS')
```


## Background
The report is generated to identify patterns of activities when people perfom excercises. To this end, a random forest model has been constructed using the `caret` r package. The data used in this analsyis were collected from the experiment, in which 6 participants were asked to perform unilateral dumbbell biceps curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). The human activity recognition device recorded data from accelerometers on the belt, forearm, arm, and dumbell respectively. 

Read more: [http://groupware.les.inf.puc-rio.br/har#ixzz49bb9umaP](http://groupware.les.inf.puc-rio.br/har#ixzz49bb9umaP)





## Data Source and Acquisition
The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. The train and test datasets are available from the weblinks as follows: 

Train: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")

Test: [https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv]("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"). 

```{r}
## Obtain train and test datasets 
train = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
test = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
```

## Data Preparation and Feature Selection
```{r ref.label= 'data prep', include=F}

```
The original train dataset contains `r dim(train)[2]` variables, but unfortunately, many features recorded in this project are not useful for building a random forest model due to the high percentatge of NA and empty values. Besides, the index varaible - X and response variable - classe(Y) should also be excluded to create predictor features(x). After clearning up, the predictors to be used for model training are: *`r names(tr_x)`*  

```{r data prep}
# Find the variable columes that contain more that 80% NAs or empty values. Those data may probably not be useful for modeling due to the limited info and varaibility they bear.  
setDT(train)
setDT(test)
na_gt_80perc = train[, lapply(.SD, function(c) ((is.na(c) | c == "" ) %>% sum)/length(c)) > 0.8]

# Remove the index variable - X, response varaible(Y) - classe and the problem id - problem_id from train and test datasets 
tr_x = train[, c(!na_gt_80perc), with = F] %>% select(-classe, -X) 
te_x = test[, c(!na_gt_80perc), with = F] %>% select(-problem_id, -X) 
tr_Y = train[, c(!na_gt_80perc), with = F]$classe
```


## Train a random forest model
In order to predict the pattern of acitvities when the particpants did the exercise, a random forest model is trainied using a subset of the train dataset. The remaining data will be used to estimate the out of sample error. Briefly, the above modified train dataset is further split into sub_train and sub_test using the `createDataPartition` function in the `caret` package. The sub_train dataset is then used to train a random forest model with 5 fold cross validation to minimize the overfitting problem. Besides, a multicore processing is also applied by allowing the paralle computing implemented in the `doMC` pacakge


```{r}
# Data preperation
intrain = createDataPartition(train$X, p = .75) %>% unlist
sub_train_x = tr_x[intrain, ]
sub_train_Y = tr_Y[intrain]
sub_test_x = tr_x[-intrain, ]
sub_test_Y = tr_Y[-intrain]

# Register multi-cores using DoMC package 
registerDoMC(cores = detectCores() - 1)  

# Train a random forest model  
trCtl = trainControl(method = "cv", number = 5, allowParallel = T) 
# mod = train(sub_train_x, sub_train_Y, method = 'rf', trControl = trCtl )
```

## Model interpretation 
The random forest model, unlike linear regression model, is trained on bagged data using randomly selected featrues to build a large number of decision trees. This makes internal logic for decision process like a black box. To get intrinsic insight into the constructed random forest model, the order of variable importance is shown as follows: 

```{r}
mod
varImp(mod)
varImpPlot(mod$finalModel, type = 2)
```

## Prediction on the sub_test dataset
```{r ref.label= 'prediction', include=F}

```

I further predict the outcomes of sub_test dataset and estmate the out of sample error using `confusionMatrix` in caret package. As shown below, the trained random forest model successfully predicts the `classe` variable with an overall accuracy of 0.999. 

```{r prediction}
# Prediction
sub_test_pred = predict(mod, sub_test_x)

# Confusion Matrix
confusionMatrix(sub_test_pred, sub_test_Y) 
```


## Prediction on the test dataset
```{r}
# Complement the lacking level information for the 20 test cases
setDF(te_x); setDF(tr_x)
for(i in seq_along(tr_x)) {levels(te_x[, i]) = levels(tr_x[, i])}

# Prediction on the test dataset      
test.pred = predict(mod, newdata = te_x)
```
*Note: In order to comply with Coursera's Honor Code, the results of the prediection have not been shown intentionally.*
